大家好，本次分享的论文是：Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei A. F. Florêncio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo: TAP: Text-Aware Pre-training forText-VQA andText-Caption.CoRRabs/2012.04638(2020)



## 背景以及相关工作

**Text-VQA**和**Text-Caption**是两个和**图像中文字(scene text)**信息相关的多模态任务。Text-VQA旨在回答跟图像中文字相关的问题，输入是一张图片和一个针对图像中文字的问题，比如“what is the company name?“，模型需要输出答案（一个词或一个词组）。Text-Caption旨在对一张图片生成一句涵盖了图像中文字信息的描述，输入是一张图片，输出是一句话，比如 ”a book titled Harry Potter“。**对于这两个任务来说，理解图像中文字和图像内容的语义关系都是至关重要的。预训练**的相关工作目前在各类领域都有很好的效果，然而现有的预训练任务都不能指导模型关注**图像中的文本信息**，以及**图像中的文本信息和图像内容的关系**。为此，本文提出了针对图像中文本信息的预训练任务，在下游任务Text-VQA和Text-Caption上都取得了最好的效果。

## 模型结构

本文的预训练模型的结构十分简单，由3层的文本编码器（使用Bert-base的前三层初始化）和4层的多模态编码器（Fusion Module）组成，每一层都是transformer layer的结构。这部分结构和TextVQA任务中的M4C[1]模型的encoder保持一致。模型的输入包括文本特征序列 ![[公式]](https://www.zhihu.com/equation?tex=w) 、对象视觉特征序列 ![[公式]](https://www.zhihu.com/equation?tex=v^{obj}) 、文字视觉特征序列 ![[公式]](https://www.zhihu.com/equation?tex=v^{ocr}) 三部分，如图1 所示。其中文本特征序列 ![[公式]](https://www.zhihu.com/equation?tex=w%3D[w^q%2C+w^{obj}%2C+w^{ocr}]) ， ![[公式]](https://www.zhihu.com/equation?tex=w^q) 指的是问题或者图像描述的token序列， ![[公式]](https://www.zhihu.com/equation?tex=w^{obj}) 指的是图像中用Faster R-CNN识别出来的对象的类别序列， ![[公式]](https://www.zhihu.com/equation?tex=w^{ocr}) 指的是通过OCR检测模型得到的图像中的文字序列。 ![[公式]](https://www.zhihu.com/equation?tex=v%5E%7Bobj%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=v%5E%7Bocr%7D) 则分别是基于Faster-RCNN和OCR检测模型得到的bounding boxes抽取的图像视觉特征序列。文本特征序列 ![[公式]](https://www.zhihu.com/equation?tex=w) 会先经过文本编码器得到token embedding，然后送入Fusion Module中，视觉特征序列 ![[公式]](https://www.zhihu.com/equation?tex=v%5E%7Bobj%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=v%5E%7Bocr%7D) 则分别经过一层全连接层，然后送入Fusion Module中。此外还有一个特殊的token <begin>也会同时输入，用于预测图文是否匹配。

![img](https://pic3.zhimg.com/80/v2-aebe7b20e5d60de8eef35732f9df383a_1440w.jpg)图1. 预训练模型以及任务

## 预训练任务

文本的预训练任务主要分成两类：1）scene-text language pre-training tasks: 即图像中文字相关的**涉及视觉和文本**的多模态预训练任务；2）scene-text visual pre-training tasks: 即图像中文字相关的但**只涉及视觉**的预训练任务

- **Scene-text Language Pre-training tasks**

对于这部分预训练任务，作者沿用了经典的visual-language pretraining的任务**ITM**（image-text matching）以及**MLM**（masked language modeling）。在ITM中，本文的做法稍微有点不同的是，在构造负例时，**不会替换整个文本序列 ![[公式]](https://www.zhihu.com/equation?tex=w) ，而是替换 ![[公式]](https://www.zhihu.com/equation?tex=w) 中的任意一个子序列**（ 即![[公式]](https://www.zhihu.com/equation?tex=w^q) ， ![[公式]](https://www.zhihu.com/equation?tex=w^{obj}) 或者 ![[公式]](https://www.zhihu.com/equation?tex=w^{ocr}) ）。在MLM中，本文对 ![[公式]](https://www.zhihu.com/equation?tex=w) 采用了惯用的mask策略（15%mask, 其中80%换成<mask>，10%换成一个随机的词，10%不做修改），稍微不同的是，**本文引入了 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) 作为文本序列的一部分**，相当于增加了图像中的文字被mask的概率，这会使得模型更关注图像中文字部分的信息。

- **Scene-text Visual Pre-training tasks**

在Text-VQA和Text-Caption中，建模**图像中对象和图像中文字的空间关系**往往能帮助理解两者之间的语义关系，例如一本书的封面上方往往是书名而不是出版社。因此，本文提出了一个专门预测空间关系的任务**RPP**（relative position prediction）。该任务中，随机选取fusion module输出的一个对象视觉特征 ![[公式]](https://www.zhihu.com/equation?tex=f^{obj}_{i}) 和文字视觉特征 ![[公式]](https://www.zhihu.com/equation?tex=f^{ocr}_j) ，通过全连接层，预测两者之间的相对位置关系，如图1所示。相对位置关系类别在该任务的开始阶段采用2分类（即是否包含），之后拓展到12分类，包括on，cover，overlap， eight-way relative oriention和unrelated。

## 下游任务

预训练阶段的模型和任务设置旨在得到更好的多模态特征表示，为了适应TextVQA和Text-Caption，模型需要增加额外的结构并在下游任务上finetune。TextVQA和Text-Caption两个任务虽然输出结果不同，一个是简短的回答，一个是图像的描述，但是都是生成任务，因此都需要在预训练模型的基础上增加一个解码器。本文在两个下游任务上都采用了统一的模型结构M4C[1]，如图2所示。M4C的编码器和预训练阶段的模型一致，解码器采用了一个全连接层和一个指针网络（Pointer Network），前者将特征映射到一个常用词词典上，后者将特征映射到对应图像包含的文本的词典上，以解决图像中文字的OOV（out-of-vocabulary）问题。两个任务采用一致的模型结构，但是需要注意的是，对于Text-Caption任务来说，输入是没有文本序列中的子序列 ![[公式]](https://www.zhihu.com/equation?tex=w^q) 的，因此这部分在finetune和测试的时候都是空白。

![img](https://pic4.zhimg.com/80/v2-6231c669debfbb84bbfdd8f55124cf4f_1440w.jpg)图2. 下游任务finetune模型结构

## 数据集

本文在**Text-VQA任务上**采用了两个数据集**TextVQA**[2]（2.8w图，4.5w问题）和**ST-VQA**[3]（2.1w图，3.1w问题）；在**Text-Caption任务上**采用了一个数据集**TextCaps**[4]（2.8w图, 14w 图像描述）。这几个数据集对于预训练来说都不够大，为此本文额外构建了一个140w的包含文字信息的图文对数据集OCR-CC。该数据集来源于Conceptual Cpationing(CC)[5]数据集，本文通过OCR识别以及一些过滤规则筛选出其中包含文字信息的图片。**注意，这些图片的描述不一定包含图片中的文字信息。**OCR-CC只用于预训练，不用于finetune和测试。

## 实验结果与分析

图3展示了本文的预训练模型在TextVQA数据集的效果。TAP表示本文的预训练模型，并且只使用下游数据TextVQA做预训练， ![[公式]](https://www.zhihu.com/equation?tex={\rm+TAP}^{\dagger\dagger}) 表示使用了所有数据集做预训练。相比M4C，TAP额外使用了一段OCR文本序列 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) 作为输入，因此为了公平对比，作者设计了 ![[公式]](https://www.zhihu.com/equation?tex=M4C^{\dagger}) 。（1）首先对比TAP和 ![[公式]](https://www.zhihu.com/equation?tex=M4C%5E%7B%5Cdagger%7D) ，在使用相同训练数据和训练步数的条件下，TAP有明显的提升，这说明本文的预训练任务可以有效地辅助模型学习图像对象和图像文字的关系；（2）在使用了更多数据做预训练的设置下， ![[公式]](https://www.zhihu.com/equation?tex=%7B%5Crm+TAP%7D%5E%7B%5Cdagger%5Cdagger%7D) 相比TAP又有了明显的提升，说明更大的数据量对于预训练是很有帮助的。本文在ST-VQA和TextCaps数据集上都有相似的结论，这里不再赘述。

![img](https://pic3.zhimg.com/80/v2-4ec62114e1bd31ed0fdc67aed787dd42_1440w.jpg)图3. TextVQA上效果对比。

为了进一步探究各个任务对预训练的影响，作者在TextVQA数据集上做了针对预训练任务的消融实验，结果如图4所示。Non-TAP即上面提到的 ![[公式]](https://www.zhihu.com/equation?tex=M4C%5E%7B%5Cdagger%7D) ，w/o ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) 表示不再输入 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) （Non-TAP w/o ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) 即M4C模型）。（1）对比b和d，发现两个引入了 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) 的多模态任务MLM和ITM对效果有明显提升；（2）对比a和c（或者b和c），发现引入一般的MLM和ITM对Text-VQA任务并没有帮助，这说明额外引入 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7Bocr%7D) 的重要性；（3）对比d和f（或者b和e），发现针对相对位置关系的RPP任务也能带来较明显的提升。

![img](https://pic3.zhimg.com/80/v2-3a7ceae8df659198fa05bf0533cab0b6_1440w.jpg)图4. 针对预训练任务的消融实验

为了进一探究数据量对预训练的影响，作者同样在TextVQA数据集上做了针对预训练数据量的消融实验，如图5所示。(3，4) 和（10, 12）代表不同大小的TAP模型，例如（10，12）表示文本编码器有10层，fusion module有12层。（1）纵向对比，从a到e，预训练数据量逐渐提升，对于两个模型来说，准确率都在逐步提升，说明数据量越大，预训练模型效果越好；（2）横向对比，对于a,b,c来说，小模型效果优于大模型，但对于d,e来说，小模型的效果差于大模型，这说明大模型需要更多的数据量来发挥它的潜力。

![img](https://pic2.zhimg.com/80/v2-3d781d7087de9baf89fb93a4dd3753f9_1440w.jpg)图5. 针对预训练数据量的消融实验

此外，作者还通过计算共指得分（coreference score）来验证引入了预训练之后，模型能更好得将文本和图像中的文字区域，图像中的对象区域和图像中的文字区域对应起来。coreference score指的是self-attention过程中两个相关的多模态特征的注意力得分，如图6所示。

![img](https://pic1.zhimg.com/80/v2-84f2fa4d84699840427dbbb28d85b768_1440w.jpg)图6. coreference score计算示例图

对于文本token 'coors'，它真实对应的区域是视觉特征序列的最后一个，那么以‘coor’作为key做完self-attenton的操作之后，最后一个视觉特征的attention score即为text word('coor')->scene text region的共指得分，共指得分越高，代表模型关联两种模态的能力越强。对比不引入预训练的模型，共指得分的结果如图7所示。可见，引入了预训练之后，模态之前的对齐能力有了明显提升。

![img](https://pic3.zhimg.com/80/v2-f296c0a69d79926affd07aafc4e83086_1440w.jpg)图7. coreference score结果。

## 总结

针对特定的下游任务，本文设计了**特定的预训练任务**，取得了不错的效果。并且本文验证了即使是**小规模的数据，也可以同样采用预训练任务**来提升模型的效果。

## 参考文献

[1]Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Marcus Rohrbach. Iterative answer prediction with pointer augmented multimodal transformers for textvqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9992–10002, 2020. 1, 2, 4, 5, 6, 12, 13

[2]Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, pages 8317–8326, 2019. 1, 2, 4, 5, 6, 12

[3]Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,Marc¸al Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, pages 4291–4301, 2019. 1, 2, 4, 6, 12

[4]Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh: TextCaps: A Dataset for Image Captioning with Reading Comprehension. ECCV (2) 2020: 742-758

[5]Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, pages 2556–2565, 2018. 2, 4, 12